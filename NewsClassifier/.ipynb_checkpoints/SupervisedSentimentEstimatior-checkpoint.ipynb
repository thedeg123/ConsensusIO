{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "data_path = './datasets/supervised/meta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "import numpy as np\n",
    "with open(data_path, 'rb') as f:\n",
    "    data_raw = pickle.load(f)\n",
    "X,y = np.array(data_raw['X']), np.array(data_raw['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import BaseEstimator, TransformerMixin, Pipeline\n",
    "import urlextract\n",
    "import re\n",
    "class Cleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, include_subj=True, replace_html=True, remove_punctuation=True, replace_urls=True, replace_numbers=True):\n",
    "        self.include_subj = include_subj; self.replace_html = replace_html\n",
    "        self.remove_punctuation = remove_punctuation; self.replace_urls = replace_urls;\n",
    "        self.replace_numbers = replace_numbers  \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def __html_to_plain_text__(self, html: str) -> str:\n",
    "        from bs4 import BeautifulSoup\n",
    "        return BeautifulSoup(html, 'html.parser').get_text()\n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = []\n",
    "        for article in X:\n",
    "            text = \" \".join(article) if self.include_subj else \" \".join(article[1:])\n",
    "            if self.replace_html:\n",
    "                text = self.__html_to_plain_text__(text)\n",
    "            if self.replace_urls:\n",
    "                url_extractor = urlextract.URLExtract() \n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" URL \")\n",
    "            if self.replace_numbers:\n",
    "                text = re.sub(r'\\d+(?:\\.\\d*(?:[eE]\\d+))?', ' NUMBER ', text)\n",
    "            if self.remove_punctuation:\n",
    "                text = text.replace(\"\\'\", \"\").replace(\"â€™\", \"\") #Because we dont want these to be replaced by spaces\n",
    "                text = re.sub(r'\\W+', ' ', text, flags=re.M)\n",
    "            X_transformed.append(text)\n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "class CountVectorizerWithStemming(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        lemm = WordNetLemmatizer()\n",
    "        analyzer = super(CountVectorizerWithStemming, self).build_analyzer()\n",
    "        return lambda doc: (lemm.lemmatize(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class filter_to_fin():\n",
    "    def __init__(self, model_path:str = './models/fin_not_fin.model', inverse_recall_rate:float = 0.4):\n",
    "        self.model_path = model_path\n",
    "        self.inverse_recall_rate = inverse_recall_rate\n",
    "    def transform(self, X, y=None):\n",
    "        with open(self.model_path, 'rb') as f:\n",
    "            m_t = pickle.load(f)\n",
    "        #we want a pretty lose recall here, as most of the things coming will be financial news\n",
    "        indexes = m_t['model'].predict_proba(m_t['transformer'].transform(X))[:,1]>self.inverse_recall_rate\n",
    "        return X[indexes], y[indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seeding guided LDA\n",
    "bias_list = ['jumped', 'hike', 'trend-line', 'earnings','candle','ipo', 'fibonacci', 'sma', 'rise', 'growth',\\\n",
    "             'bulls', 'bears', 'bullish', 'optimistic', 'rally', 'surge', \\\n",
    "                'soared', 'growth' 'buy', 'higher', 'gains', 'outperform','lower',\\\n",
    "             'slumped', 'fell', 'worry', 'bearish', 'miss', 'sell', 'losses', 'warn', \\\n",
    "                 'plummet', 'bad', 'down', 'low', 'disappointed', 'weak', 'worry']\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "class SetBias(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, bias_list:[str], bias_strength:float):\n",
    "        self.bias_list = bias_list\n",
    "        self.bias_strength = bias_strength\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X, y=None):\n",
    "        if type(X) is csr_matrix:\n",
    "            X = X.toarray()\n",
    "        return csr_matrix(np.add(X, data_transformer.transform([self.bias_list]).toarray()*self.bias_strength, where=X!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import exceptions\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "cvws = CountVectorizerWithStemming(stop_words=\"english\", max_features=20_000)\n",
    "data_transformer = Pipeline([\n",
    "    ('clean', Cleaner()), #cleans text\n",
    "    ('vect', cvws), #turns words to counts \n",
    "    ('tfidf', TfidfTransformer()), #turns counts to tf-idf\n",
    "])\n",
    "bias_set = Pipeline([\n",
    "    ('bias', SetBias(bias_list, bias_strength = 1.2)), #setting bias for values\n",
    "])\n",
    "all_transform = Pipeline([\n",
    "    ('data_trans', data_transformer),\n",
    "    ('bias_trans', bias_set)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<1160x7781 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 39727 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 391,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#using the fin_not_fin model to filter out non-financial news\n",
    "X_filtered, y_filtered = filter_to_fin(inverse_recall_rate=0.4).transform(X_train,y_train)\n",
    "#preprocessing the data\n",
    "X_train_prepared = all_transform.fit_transform(X_filtered)\n",
    "X_train_prepared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74.40787935648589+=4.429257532954491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "log_clf = LogisticRegression(solver=\"liblinear\", random_state=42, n_jobs=-1)\n",
    "score = cross_val_score(grid_search_svc.best_estimator_, X_train_prepared , y_filtered, cv=20, verbose=1)\n",
    "print(\"{}+={}\".format(100*score.mean(), 100*score.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1160\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(X_filtered))\n",
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 1 errors:\n",
      "american horror novelist alan wake finds unlikely new publisher, finnish video game company remedy entertainment\n",
      "Type 2 \"errors\":504\n"
     ]
    }
   ],
   "source": [
    "a = set()\n",
    "for x_title in X_filtered:\n",
    "    a.add(x_title[1])\n",
    "print(\"Type 1 errors:\")\n",
    "for (i,x_f_title) in enumerate(X_train):\n",
    "    if x_f_title[1] not in a and (y_train[i]!=1):\n",
    "        print(x_f_title[1])\n",
    "print(\"Type 2 \\\"errors\\\":{}\".format(len(y_filtered[y_filtered==1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from xgboost import XGBClassifier\n",
    "log_clf = LogisticRegression(solver=\"liblinear\",C = 1.4, dual= False, penalty = 'l2', random_state=42, n_jobs=-1)\n",
    "lin_svc = LinearSVC(random_state=42)\n",
    "#lin_svc = SVC(kernel='linear',probability=True)\n",
    "#boost_clf = XGBClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "#extra_clf = ExtraTreesClassifier(n_estimators=100, max_leaf_nodes=16, n_jobs=-1) #random thresholds set\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators = [('log_reg', log_clf), ('forest_clf', forest_clf), ('sgd_clf', lin_svc)],\n",
    "    voting = 'hard',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.7184048821548822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC 0.7376515151515152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    3.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 0.6929611592111591\n",
      "VotingClassifier 0.7375841750841752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    2.9s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from warnings import filterwarnings as warn\n",
    "warn(\"ignore\", category=DeprecationWarning)\n",
    "for clf in (log_clf, lin_svc, forest_clf, voting_clf):\n",
    "    score = cross_val_score(clf, X_train_prepared , y_filtered, cv=20, verbose=1)\n",
    "    print(clf.__class__.__name__, score.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method LinearClassifierMixin.decision_function of LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=42, tol=0.0001,\n",
       "     verbose=0)>"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_svc.decision_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 29 candidates, totalling 580 fits\n",
      "Acheaved score of:\t 0.7482758620689656\n",
      "With following paramaters:\t {'C': 0.4, 'dual': False, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 580 out of 580 | elapsed:    9.4s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "#Optimizing hyperparams for svc\n",
    "param_grid = [\n",
    "    {'C': np.arange(0.1, 1.5,0.1), 'dual': [False, True], 'penalty': ['l2']},\n",
    "    {'C': np.arange(0.4, 0.5,0.1), 'dual': [False], 'penalty': ['l1']},\n",
    "]\n",
    "grid_search_svc = GridSearchCV(lin_svc, param_grid, cv=20, return_train_score = True, verbose=1)\n",
    "grid_search_svc.fit(X_train_prepared, y_filtered)\n",
    "print(\"Acheaved score of:\\t\", grid_search_svc.best_score_)\n",
    "print(\"With following paramaters:\\t\", grid_search_svc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 22 candidates, totalling 440 fits\n",
      "Acheaved score of:\t 0.7377495462794919\n",
      "With following paramaters:\t {'C': 2.500000000000001, 'dual': False, 'penalty': 'l2'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 440 out of 440 | elapsed:    6.4s finished\n"
     ]
    }
   ],
   "source": [
    "#Optimizing hyperparams for log reg\n",
    "param_grid = [\n",
    "    {'C': np.arange(1.8,4,0.1), 'dual': [False], 'penalty': ['l2']},\n",
    "]\n",
    "grid_search_log = GridSearchCV(log_clf, param_grid, cv=10, return_train_score = True, verbose=1)\n",
    "grid_search_log.fit(X_train_prepared, y_filtered)\n",
    "print(\"Acheaved score of:\\t\", grid_search_log.best_score_)\n",
    "print(\"With following paramaters:\\t\", grid_search_log.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed: 19.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acheaved score of:\t 0.7323049001814882\n",
      "With following paramaters:\t {'n_estimators': 1200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': False}\n"
     ]
    }
   ],
   "source": [
    "#Optimizing hyperparams for random forest\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "param_grid = {'bootstrap': [True, False],\n",
    " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n",
    " 'max_features': ['auto', 'sqrt'],\n",
    " 'min_samples_leaf': [1, 2, 4],\n",
    " 'min_samples_split': [2, 5, 10],\n",
    " 'n_estimators': np.arrange(200,2200,200)}\n",
    "\n",
    "random_search_forest = RandomizedSearchCV(forest_clf, param_grid, cv=10, return_train_score = True, verbose=1)\n",
    "random_search_forest.fit(X_train_prepared, y_filtered)\n",
    "print(\"Acheaved score of:\\t\", random_search_forest.best_score_)\n",
    "print(\"With following paramaters:\\t\", random_search_forest.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [grid_search_log.best_estimator_, \\\n",
    "          random_search_forest.best_estimator_, \\\n",
    "          grid_search_svc.best_estimator_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1039: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  if hasattr(X, 'dtype') and np.issubdtype(X.dtype, np.float):\n"
     ]
    }
   ],
   "source": [
    "X_test_filtered, y_test_filtered = filter_to_fin().transform(X_test, y_test)\n",
    "X_test_prepared = all_transform.transform(X_test_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task:         [classification]\n",
      "n_classes:    [3]\n",
      "metric:       [accuracy_score]\n",
      "mode:         [oof_pred_bag]\n",
      "n_models:     [3]\n",
      "\n",
      "model  0:     [LogisticRegression]\n",
      "    fold  0:  [0.73195876]\n",
      "    fold  1:  [0.79310345]\n",
      "    fold  2:  [0.70689655]\n",
      "    fold  3:  [0.71280277]\n",
      "    ----\n",
      "    MEAN:     [0.73619038] + [0.03413983]\n",
      "    FULL:     [0.73620690]\n",
      "\n",
      "model  1:     [RandomForestClassifier]\n",
      "    fold  0:  [0.70790378]\n",
      "    fold  1:  [0.77586207]\n",
      "    fold  2:  [0.70344828]\n",
      "    fold  3:  [0.70242215]\n",
      "    ----\n",
      "    MEAN:     [0.72240907] + [0.03092982]\n",
      "    FULL:     [0.72241379]\n",
      "\n",
      "model  2:     [LinearSVC]\n",
      "    fold  0:  [0.73195876]\n",
      "    fold  1:  [0.78965517]\n",
      "    fold  2:  [0.70689655]\n",
      "    fold  3:  [0.72318339]\n",
      "    ----\n",
      "    MEAN:     [0.73792347] + [0.03119169]\n",
      "    FULL:     [0.73793103]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from vecstack import stacking\n",
    "S_train, S_test = stacking(models,                   \n",
    "                           X_train_prepared, y_filtered, X_test_prepared,   \n",
    "                           regression=False, \n",
    "                           mode='oof_pred_bag', \n",
    "                           needs_proba=False,\n",
    "                           save_dir=None, \n",
    "                           metric=accuracy_score,\n",
    "                           n_folds=4, \n",
    "                           stratified=True,\n",
    "                           shuffle=False,  \n",
    "                           random_state=42,    \n",
    "                           verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1102, 3)"
      ]
     },
     "execution_count": 339,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 10 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   21.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acheaved score of:\t 0.7482758620689656\n",
      "With following paramaters:\t {'n_estimators': 400, 'max_depth': 3, 'learning_rate': 0.51}\n"
     ]
    }
   ],
   "source": [
    "modelStacking = XGBClassifier(random_state=42, n_jobs=-1)\n",
    "param_grid = {'learning_rate':np.arange(0.01, 1, 0.1), \n",
    "              'n_estimators':np.arange(50,500,50), \n",
    "              'max_depth':[1,2,3]}\n",
    "random_search_stack = RandomizedSearchCV(modelStacking, param_grid, cv=10, return_train_score = True, verbose=1)\n",
    "random_search_stack.fit(S_train, y_filtered)\n",
    "print(\"Acheaved score of:\\t\", random_search_stack.best_score_)\n",
    "print(\"With following paramaters:\\t\", random_search_stack.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.7311537999037999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC 0.7311537999037999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier 0.7302934102934102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    0.7s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VotingClassifier 0.7311537999037999\n",
      "XGBClassifier 0.737502405002405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed:    1.3s finished\n"
     ]
    }
   ],
   "source": [
    "#best off going with the 0.74 of the linearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./models/news_sentiment.model', 'wb') as f:\n",
    "    pickle.dump({'transformer':all_transform, 'model':grid_search_svc.best_estimator_}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
