<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>About</title>
    {% load static %}
    <link rel="stylesheet" type="text/css" href="{% static 'web/plain.css' %}">
    <link rel="icon" href="../../static/web/images/logo.png">
    </head>
    <body>
        <nav id="nav-bar">
            <div class="container-nav">
                <ul> 
                    <li><a href="{% url 'web:index' %}"><img class="img" src="{% static 'web/images/text_logo.png' %}"></a>
                    </li>
                    <li><a href="{% url 'web:about' %}">About</a></li>
                    <li><a href="{% url 'web:acknowledgments' %}">Acknowledgments</a></li>
                </ul>
            </div>
        </nav>
        <div class="clr"></div>
<div id="top">
    <h1>ConsensusIO</h1>
    <h2><i>Finance Based NLP for people in a hurry.</i></h2>
</div>
<div class="container">
    <h2>What it is,</h2>
    <p>
    ConsensusIO is a finance-based news aggregation platform using natural language processing to classify news articles
    about a given security into one of four discrete classes: positive, indifferent, negative, and not financial. When
    supplied with a large batch of articles, ConsensusIO gives a pulse on markets regarding the sentiment of a given
    company.
    </p>
    <h2>How it works,</h2>
    <p>Sentiment analysis is considered a natural language processing problem. At its core, natural language processing (NLP)
    boils down into two subproblems:
    <ol>
    <li>Represent each article,<i>i</i>, in a given corpus <i>C</i>, as a vector <i>v</i>, using any manner of encoding techniques.</li>
    <li>Train a predictive model on the processed vectors to predict some new unseen article.</li>
    </ol>
    ConsensusIO relies upon two supervised natural language processing models: <i>fin_not_fin</i> and <i>news_sentiment</i>. The platforms
    logical flow can be seen below.
    </p>
    <img src="{% static 'web/images/logical_flow.png' %}"></img>
    <h2>Models and Results,</h2>
    <p>All model hyperparameters were optimized via a grid search using 20-fold cross validation. The preprocessing pipeline
    for all models consisted of word-stemming, HTML and punctuation removal, and shifting text to lower case. The train/test
    split used was 0.1.</p>
    <h3>news_sentiment</h3>
    <p>
    The <i>news_sentiment</i> model classifies financial articles into one of three discrete categories: positive, indifferent, or negative.
    For production, a SGD model with tf-idf encoding and supervised weighting was used.
    </p>
    <img src="{% static 'web/images/news_sentiment_table.png' %}">
    <h3>fin_not_fin</h3>
    <p><i>fin_not_fin</i> filters down from all news to financial news by classifying news as either financial or not financial. Due
    to the general purpose use of the NewsAPI, much of the news gathered on queries for companies such as “Amazon” are about
    hot deals rather than stock prices, thereby a model must be trained to filter out these articles. For production, a SGD
    model with tf-idf encoding and supervised weighting was used.</p>
    <img src="{% static 'web/images/fin_not_fin_table.png' %}"></img>
    <div class="notes">
    <h2>Notes</h2>
        <ol>
            <li>The verify set remained constant across all test. A set of 100 articles was used.</li>
            <li><img class="math" src="{% static 'web/images/note_one.png' %}"></img></li>
            <li>Online learning in this case consists of reviewing articles to increase the size of <i>Sp</i> and <i>Sn</i> respectively.</li>
            <li>Due to the small size and high complexity of the training set, all models performed within 0.02 accuracy of one
            another, to preserve space, are aggregated as a mean of each score.</li>
            <li>Supervised document weighting consists of considering those words known to be finance based (words in <i>Sp ∪ Sn</i>) more
            heavily as a predictive factor than other words.</li>
            <li>The SVM model was trained for high precision and a low recall while the Random Forest model was trained for low
            precision and a high recall. The logistic regression was trained for a maximal F1.</li>
            <li>Programmatic document creation:<br>
            A randomly generated article was a collection of anywhere from 10 to 80 words sampled from the following 5 sets:<br>
            <img src="{% static 'web/images/document_table.png' %}"></img>
            <br>Samples were generated at random with p(neg) = 0.4, p(pos) = 0.4 and p(ind) = 0.2. The positive/negative word banks were 
            sampled only for instances of positive and negative articles respectively. A sample randomly generated article can be seen below:<br>
            (negative)<br>
            <i>“sma because online Business Inventories view ALLE confusion at underperform Call Standard Deviation are built home
            seventeen fingering information us Consumer Price Index site fizzle Industrials Credit Spread initial use Bollinger
            Bands proc bearish been Passively Managed Funds stale poor bear out blue chip Passively Managed Funds link brace
            criticized decline also view information warn Value Stock Support business bear oldies Common Stock S&P 500 S&P 500 ten
            Capital Expenditure Allegion”</i>
            </li>
            <li>The Movie review set consisted of 5,000 negative articles and 5,000 positive articles. Data was graciously provided
            free of charge via the following paper:<br> <i>Maas, A., Daly, R., Pham, P., Huang, D., Ng, A. and Potts, C. (2011). Learning Word Vectors for Sentiment Analysis:
            Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies.
            [online] Portland, Oregon, USA: Association for Computational Linguistics, pp.142–150.</i> </li>
            <li>The breakdown of the financial vs not financial set was as follows: 5000 financial, 1667
            political, 1667 sports, 1667 technology. Data was graciously provided free of charge via WebHose.</li>
        </ol>
    <h2>Design FAQ</h2>
    <h3><i>- Why use two models as opposed to one model to classify everything?</i></h3>
        <p class="answer">Two models were used due to the prevalence of data for one problem, and the scarcity of data for the other. In the case
        of fin_not_fin, one can easily find large amounts of labeled training data; one must simply find financial news for one
        group, and any other news for the other group. Meanwhile, the news_sentiment model relied upon the highly intensive and
        laborious process of a hand labeling a training data set. Programmatic document creation was employed to lessen this task.
        </p>
    <h3><i>- The BERT model worked the best, why not use that in production?</i></h3>
        <p class="answer">While the BERT model performs the best, the computation time to retrain the model and encode data is significantly
        greater than that of simpler models. Therefore, running these models on pythonanywhere hardware would significantly slow
        down the site. While it is possible to run these models quickly and efficiently using Cloud TPUs via the Google Cloud
        API, this was not accomplished due to time constraints.
        </p>
    <h3><i>- 0.64 accuracy in the news_sentiment classifier seems pretty poor, no?</i></h3>
    <p class="answer">Yes, 0.64 accuracy is terrible! This stems from the extremely small training set to predict hyper-complex inputs with
        literally thousands of dimensions. Unfortunately, even with transfer learning, one cannot “model” their way out of a
        small dataset. This is where ConsensusIO’s incremental learning comes in. The more one corrects articles, the better
        the platform becomes! By the time you read this, the platform’s prediction is most likely better than 0.64.</p>
    <h3><i>- Why is it sometimes so slow to load pages but other times so fast?</i></h3>
        <p class="answer">
        The two main delays in loading a page come from querying the two APIs and transforming the dataset for the ML models.
        Therefore, when a company is searched, the platform stores the gathered information and ML predictions in a SQL
        database. Next time the company is queried the program can simply fetch the company from the database, rather than
        repeating the entire ML pipeline.
        </p>
    <h3><i>- How often are the models updated?</i></h3>
        <p class="answer">Models are automatically retrained at 11:50 PM (PST) every night.
        </p>
    <h3><i>- Why are the articles I see often unrelated to my search or not from today?</i></h3>
        <p class="answer">ConsensusIO relies upon the free NewsAPI for its content. While powerful, this API is in no way finance-focused.
        Therefore, much of the news received is either not financial or not important. Due to the limiting nature of
        the free API, the platform only gathers news once per day per company. Ideally in the future, the platform will
        transition to the (paid) stocknewsapi.
        </p>
    <h3><i>- Why can I see only so few articles at a time?</i></h3>
        <p class="answer">Due to the free nature of the NewsAPI, the platform is limited to 500 articles a day. Therefore to ensure some form of
        usability, the article sets are limited per API call to a minimum of 10 per batch.
        </p>
    <h3><i>- What about concept drift?</i></h3>
        <p class="answer">As of yet, ConsensusIO has no fancy solution to deal with concept drift. The new and old models are automatically tested
        on a preprocessed corpus of 100 entries. The new model is put into production only if it performs better (higher
        accuracy) on the verification corpus than the old model.
        </p>
    </div>
    <h2>Contact</h2>
    <p>ConsensusIO was developed in summer 2019 by undergraduate David Gold. You can get in touch with him here:<br>
        <a target="_blank" href="https://www.linkedin.com/in/davidegold/">LinkedIn</a><br>
        Email: thedeg123@gmail.com<br>
        The source code for ConsensusIO is availible on <a target="_blank" href="https://github.com/thedeg123/ConsensusIO">GitHub</a>. The ML notebooks responsible for the models are a mess, but also available <a target="_blank" href="https://github.com/thedeg123/NewsClassifier">in a separate repo</a>.
    </p>
</div>
<footer id="main-footer">
    <p>MIT License Copyright &copy; 2019 David Gold</p>
</footer>
</body>
</html>